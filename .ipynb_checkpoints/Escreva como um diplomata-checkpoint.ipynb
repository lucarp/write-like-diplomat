{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto: Escreva como um diplomata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    return open(filepath, 'r').read()\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    vocab = set(words)\n",
    "    \n",
    "    counts = Counter(words)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "    int_to_vocab = {ii: word for ii, word in vocab_to_int.items()}\n",
    "    \n",
    "    text_ints = []\n",
    "    for word in words:\n",
    "        text_ints.append(vocab_to_int[word])\n",
    "    \n",
    "    text_ints = np.array(text_ints)\n",
    "    labels_ints = np.zeros_like(text_ints)\n",
    "    labels_ints[:-1], labels_ints[-1] = text_ints[1:], text_ints[0]\n",
    "\n",
    "    return text_ints, labels_ints, int_to_vocab, vocab_to_int\n",
    "\n",
    "\n",
    "features, labels, int_to_vocab, vocab_to_int = preprocess(get_data('data/model.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   6,   70,   98, 2441, 3841,    2, 2442, 1508,  262,    4, 1396,\n",
       "          28,   92,  114,   12,  203,   25, 4791,    8, 3197,    5,  209,\n",
       "          30,  387,   17, 4792,    1,   17, 2172,    5,   10, 1953, 2173,\n",
       "           1,   26,    9,  121, 3198,   24, 1954,   58, 3842,    3, 1623,\n",
       "        3843,    1,  424,    2, 2174,   48, 6741,   30,  242,  120,    9,\n",
       "         294,   55, 3844,   54,    1,   12,  366,  110,  624, 6742,    5,\n",
       "         263,    2, 3845,    9, 6743,   66, 1955, 3846,    1,   25,  589,\n",
       "        3199,   28,   92,  114,   12,   16,  523,   55, 6744,   54,    1,\n",
       "          12,    4, 3200,    2,  373, 1397,    1,  253,    2,  294,    1,\n",
       "         110]),\n",
       " array([  70,   98, 2441, 3841,    2, 2442, 1508,  262,    4, 1396,   28,\n",
       "          92,  114,   12,  203,   25, 4791,    8, 3197,    5,  209,   30,\n",
       "         387,   17, 4792,    1,   17, 2172,    5,   10, 1953, 2173,    1,\n",
       "          26,    9,  121, 3198,   24, 1954,   58, 3842,    3, 1623, 3843,\n",
       "           1,  424,    2, 2174,   48, 6741,   30,  242,  120,    9,  294,\n",
       "          55, 3844,   54,    1,   12,  366,  110,  624, 6742,    5,  263,\n",
       "           2, 3845,    9, 6743,   66, 1955, 3846,    1,   25,  589, 3199,\n",
       "          28,   92,  114,   12,   16,  523,   55, 6744,   54,    1,   12,\n",
       "           4, 3200,    2,  373, 1397,    1,  253,    2,  294,    1,  110,\n",
       "         624]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:100], targets[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement get_batches to create batches of input and targets using int_text. The batches should be a Numpy array with the shape (number of batches, 2, batch size, sequence length). Each batch contains two elements:\n",
    "The first element is a single batch of input with the shape [batch size, sequence length]\n",
    "The second element is a single batch of targets with the shape [batch size, sequence length]\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "For exmple, get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2) would return a \n",
    "\n",
    "Numpy array of the following:\n",
    "\n",
    "[\n",
    "  #First Batch\n",
    "  [\n",
    "    #Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    #Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  #Second Batch\n",
    "  [\n",
    "    #Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    #Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  #Third Batch\n",
    "  [\n",
    "    #Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    #Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "  \n",
    "]\n",
    "\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, 1. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch\n",
    "def get_batches(features, targets, batch_size, seq_length):\n",
    "    assert len(features) == len(targets), \"Features and labels must have the same shape.\"\n",
    "    \n",
    "    # 1. Calculate the number of batches\n",
    "    n_elements = batch_size * seq_length\n",
    "    n_batches = len(features) // n_elements\n",
    "    \n",
    "    # 2. Trim features and targets to keep only full batches\n",
    "    # 3. Reshape features and targets to a matrix (num_of_batches, batch_size * seq_length)\n",
    "    features = np.reshape(features[:n_batches*n_elements], (n_batches, -1))\n",
    "    targets = np.reshape(targets[:n_batches*n_elements], (n_batches, -1))\n",
    "    \n",
    "    # 4. Iterate over the num of batches and reshape each batch in (batch_size X seq_length)\n",
    "    batches = []\n",
    "    for i in range(n_batches):\n",
    "        batch_of_features = features[i].reshape(batch_size,seq_length)\n",
    "        batch_of_targets = targets[i].reshape(batch_size,seq_length)\n",
    "        batches.append([batch_of_features,batch_of_targets])\n",
    "\n",
    "    return np.array(batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 128\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 14\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # Get Inputs\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate') \n",
    "    \n",
    "    # Get Init Cell\n",
    "    num_of_lstm_layers = 1\n",
    "    keep_prob = 0.3\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    dropout = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    input_data_shape = tf.shape(input_text) # esse parametro é o batch_size da função get_init_cell\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([dropout] * num_of_lstm_layers)\n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.int32), name = 'initial_state')\n",
    "    \n",
    "    \n",
    "    # Build NN\n",
    "    \n",
    "    ## Get Embed\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    n_embedding = 200 # Number of embedding features \n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_text)\n",
    "    \n",
    "    ## Build RNN\n",
    "    output, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=None, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name=\"final_state\")\n",
    "    \n",
    "    ## Fully connected layer with a linear activation and vocab_size as the number of outputs \n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        output,\n",
    "        vocab_size,\n",
    "        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "        biases_initializer=tf.zeros_initializer(),\n",
    "        activation_fn=None,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    y_one_hot = tf.one_hot(targets, vocab_size)\n",
    "    #y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    \n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"targets:0\", shape=(?, ?), dtype=int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'Tensor' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-da947d6c1b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-15b159228f38>\u001b[0m in \u001b[0;36mget_batches\u001b[0;34m(features, targets, batch_size, seq_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Features and labels must have the same shape.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 1. Calculate the number of batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Tensor' has no len()"
     ]
    }
   ],
   "source": [
    "print(targets)\n",
    "batches = get_batches(features, targets, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
